{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "triple MRInet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUCY8w-fksCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "aba7ab8f-1179-4342-8623-6242bc3e9df9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Gn-LUvcVnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import pdb\n",
        "from torch.autograd import Variable\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from sklearn import metrics\n",
        "import pdb\n",
        "from sklearn import metrics\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from sklearn import metrics\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ofw1EoMmYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(series):\n",
        "        pad = int((series.shape[2] - INPUT_DIM)/2)\n",
        "        series = series[:,pad:-pad,pad:-pad]\n",
        "        series = (series-np.min(series))/(np.max(series)-np.min(series))*MAX_PIXEL_VAL\n",
        "        series = (series - MEAN) / STDDEV\n",
        "        series = np.stack((series,)*3, axis=1)\n",
        "        series_float = torch.FloatTensor(series)\n",
        "        return series_float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikRxK7f1bm8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = models.alexnet(pretrained=True)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.squeeze(x, dim=0) # only batch size 1 supported\n",
        "        x = self.model.features(x)\n",
        "        x = self.gap(x).view(x.size(0), -1)\n",
        "        x = torch.max(x, 0, keepdim=True)[0]\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class TripleMRNet(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet18\", training=True):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        if self.backbone == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=training)\n",
        "            modules = list(resnet.children())[:-1]\n",
        "            self.axial_net = nn.Sequential(*modules)\n",
        "            for param in self.axial_net.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.axial_net = models.alexnet(pretrained=training)\n",
        "\n",
        "        if self.backbone == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=training)\n",
        "            modules = list(resnet.children())[:-1]\n",
        "            self.sagit_net = nn.Sequential(*modules)\n",
        "            for param in self.sagit_net.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.sagit_net = models.alexnet(pretrained=training)\n",
        "        \n",
        "        if self.backbone == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=training)\n",
        "            modules = list(resnet.children())[:-1]\n",
        "            self.coron_net = nn.Sequential(*modules)\n",
        "            for param in self.coron_net.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.coron_net = models.alexnet(pretrained=training)\n",
        "\n",
        "        self.gap_axial = nn.AdaptiveAvgPool2d(1)\n",
        "        self.gap_sagit = nn.AdaptiveAvgPool2d(1)\n",
        "        self.gap_coron = nn.AdaptiveAvgPool2d(1)\n",
        "       \n",
        "        if self.backbone == \"resnet18\":\n",
        "            self.classifier = nn.Linear(3*512, 1)\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.classifier = nn.Linear(3*256, 1)\n",
        "\n",
        "    def forward(self, vol_axial, vol_sagit, vol_coron):\n",
        "        vol_axial = torch.squeeze(vol_axial, dim=0)\n",
        "        vol_sagit = torch.squeeze(vol_sagit, dim=0)\n",
        "        vol_coron = torch.squeeze(vol_coron, dim=0)\n",
        "       \n",
        "        if self.backbone == \"resnet18\":\n",
        "            vol_axial = self.axial_net(vol_axial)\n",
        "            vol_sagit = self.sagit_net(vol_sagit)\n",
        "            vol_coron = self.coron_net(vol_coron)\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            vol_axial = self.axial_net.features(vol_axial)\n",
        "            vol_sagit = self.sagit_net.features(vol_sagit)\n",
        "            vol_coron = self.coron_net.features(vol_coron)\n",
        "\n",
        "        vol_axial = self.gap_axial(vol_axial).view(vol_axial.size(0), -1)\n",
        "        x = torch.max(vol_axial, 0, keepdim=True)[0]\n",
        "        vol_sagit = self.gap_sagit(vol_sagit).view(vol_sagit.size(0), -1)\n",
        "        y = torch.max(vol_sagit, 0, keepdim=True)[0]\n",
        "        vol_coron = self.gap_coron(vol_coron).view(vol_coron.size(0), -1)\n",
        "        z = torch.max(vol_coron, 0, keepdim=True)[0]\n",
        "\n",
        "        w = torch.cat((x, y, z), 1)\n",
        "        out = self.classifier(w)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VI6cA9wDvDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MRI_alex(nn.Module):\n",
        "  def __init__(self, training=True):\n",
        "    super().__init__()\n",
        "    self.axial_net = models.alexnet(pretrained=training)\n",
        "    self.sagit_net = models.alexnet(pretrained=training)\n",
        "    self.coron_net = models.alexnet(pretrained=training)\n",
        "\n",
        "    self.gap_axial = nn.AdaptiveAvgPool2d(1)\n",
        "    self.gap_sagit = nn.AdaptiveAvgPool2d(1)\n",
        "    self.gap_coron = nn.AdaptiveAvgPool2d(1)\n",
        "    self.classifier = nn.Linear(3*256, 1)\n",
        "\n",
        "  def forward(self,vol_axial, vol_sagit, vol_coron):\n",
        "    vol_axial = torch.squeeze(vol_axial, dim=0)\n",
        "    vol_sagit = torch.squeeze(vol_sagit, dim=0)\n",
        "    vol_coron = torch.squeeze(vol_coron, dim=0)\n",
        "\n",
        "    vol_axial = self.axial_net.features(vol_axial)\n",
        "    vol_sagit = self.sagit_net.features(vol_sagit)\n",
        "    vol_coron = self.coron_net.features(vol_coron)\n",
        "\n",
        "    vol_axial = self.gap_axial(vol_axial).view(vol_axial.size(0), -1)\n",
        "    x = torch.max(vol_axial, 0, keepdim=True)[0]\n",
        "\n",
        "    vol_sagit = self.gap_sagit(vol_sagit).view(vol_sagit.size(0), -1)\n",
        "    y = torch.max(vol_sagit, 0, keepdim=True)[0]\n",
        "\n",
        "    vol_coron = self.gap_coron(vol_coron).view(vol_coron.size(0), -1)\n",
        "    z = torch.max(vol_coron, 0, keepdim=True)[0]\n",
        "\n",
        "    w = torch.cat((x, y, z), 1)\n",
        "    out = self.classifier(w)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cVk6Ep0b2nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = 224\n",
        "MAX_PIXEL_VAL = 255\n",
        "MEAN = 58.09\n",
        "STDDEV = 49.73\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, datadir, tear_type, use_gpu):\n",
        "        super().__init__()\n",
        "        self.use_gpu = use_gpu\n",
        "        label_dict = {}\n",
        "        self.paths = []\n",
        "        abnormal_label_dict = {}\n",
        "        if datadir[-1]==\"/\":\n",
        "            datadir = datadir[:-1]\n",
        "        self.datadir = datadir\n",
        "        for i, line in enumerate(open(datadir+'-'+tear_type+'.csv').readlines()):\n",
        "            line = line.strip().split(',')\n",
        "            filename = line[0]\n",
        "            label = line[1]\n",
        "            label_dict[filename] = int(label)\n",
        "        \n",
        "\n",
        "        for i, line in enumerate(open(datadir+'-'+\"abnormal\"+'.csv').readlines()):\n",
        "            line = line.strip().split(',')\n",
        "            filename = line[0]\n",
        "            label = line[1]\n",
        "            abnormal_label_dict[filename] = int(label)\n",
        "\n",
        "        for filename in os.listdir(os.path.join(datadir, \"axial\")):\n",
        "            if filename.endswith(\".npy\"):\n",
        "                self.paths.append(filename)\n",
        "        \n",
        "        self.labels = [label_dict[path.split(\".\")[0]] for path in self.paths]\n",
        "        self.abnormal_labels = [abnormal_label_dict[path.split(\".\")[0]] for path in self.paths]\n",
        "\n",
        "        if tear_type != \"abnormal\":\n",
        "            temp_labels = [self.labels[i] for i in range(len(self.labels)) if self.abnormal_labels[i]==1]\n",
        "            neg_weight = np.mean(temp_labels)\n",
        "        else:\n",
        "            neg_weight = np.mean(self.labels)\n",
        "        \n",
        "        self.weights = [neg_weight, 1 - neg_weight]\n",
        "\n",
        "    def weighted_loss(self, prediction, target):\n",
        "        weights_npy = np.array([self.weights[int(t[0])] for t in target.data])\n",
        "        weights_tensor = torch.FloatTensor(weights_npy)\n",
        "        if self.use_gpu:\n",
        "            weights_tensor = weights_tensor.cuda()\n",
        "        loss = F.binary_cross_entropy_with_logits(prediction, target, weight=Variable(weights_tensor))\n",
        "        return loss\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.paths[index]\n",
        "        vol_axial = np.load(os.path.join(self.datadir, \"axial\", filename))\n",
        "        vol_sagit = np.load(os.path.join(self.datadir, \"sagittal\", filename))\n",
        "        vol_coron = np.load(os.path.join(self.datadir, \"coronal\", filename))\n",
        "\n",
        "        # axial\n",
        "        vol_axial_tensor = preprocess(vol_axial)\n",
        "        \n",
        "        # sagittal\n",
        "        vol_sagit_tensor = preprocess(vol_sagit)\n",
        "\n",
        "        # coronal\n",
        "        vol_coron_tensor = preprocess(vol_coron)\n",
        "\n",
        "        label_tensor = torch.FloatTensor([self.labels[index]])\n",
        "\n",
        "        return vol_axial_tensor, vol_sagit_tensor, vol_coron_tensor, label_tensor, self.abnormal_labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "def load_data(task, use_gpu):\n",
        "    train_dir = \"/content/drive/My Drive/MRNet-v1.0/train\"\n",
        "    valid_dir = \"/content/drive/My Drive/MRNet-v1.0/valid\"\n",
        "    \n",
        "    train_dataset = Dataset(train_dir, task, use_gpu)\n",
        "    valid_dataset = Dataset(valid_dir, task, use_gpu)\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=1, num_workers=11, shuffle=True)\n",
        "    valid_loader = data.DataLoader(valid_dataset, batch_size=1, num_workers=11, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2F9XbQrcQYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, loader, train=False, optimizer=None,abnormal_model_path=None):\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        if abnormal_model_path:\n",
        "            abnormal_model = MRI_alex(train)\n",
        "            state_dict = torch.load(abnormal_model_path)\n",
        "            abnormal_model.load_state_dict(state_dict)\n",
        "            abnormal_model.cuda()\n",
        "            abnormal_model.eval()\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in tqdm(loader):\n",
        "        vol_axial, vol_sagit, vol_coron, label, abnormal = batch\n",
        "        if train:\n",
        "            if abnormal_model_path and not abnormal:\n",
        "                continue\n",
        "            optimizer.zero_grad()\n",
        "        if loader.dataset.use_gpu:\n",
        "            vol_axial, vol_sagit, vol_coron = vol_axial.cuda(), vol_sagit.cuda(), vol_coron.cuda()\n",
        "            label = label.cuda()\n",
        "        vol_axial, vol_sagit, vol_coron = Variable(vol_axial), Variable(vol_sagit), Variable(vol_coron)\n",
        "        label = Variable(label)\n",
        "\n",
        "        logit = model.forward(vol_axial, vol_sagit, vol_coron)\n",
        "\n",
        "        loss = loader.dataset.weighted_loss(logit, label)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pred = torch.sigmoid(logit)\n",
        "\n",
        "        pred_npy = pred.data.cpu().numpy()[0][0]\n",
        "\n",
        "        if abnormal_model_path and not train:\n",
        "            abnormal_logit = abnormal_model.forward(vol_axial,vol_sagit,vol_coron)\n",
        "            abnormal_pred = torch.sigmoid(abnormal_logit)\n",
        "            abnormal_pred_npy = abnormal_pred.data.cpu().numpy()[0][0]\n",
        "            pred_npy = pred_npy * abnormal_pred_npy\n",
        "\n",
        "        label_npy = label.data.cpu().numpy()[0][0]\n",
        "\n",
        "        preds.append(pred_npy)\n",
        "        labels.append(label_npy)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    \n",
        "    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    if abnormal_model_path and not train:\n",
        "        del abnormal_model\n",
        "\n",
        "    return avg_loss, auc, preds, labels\n",
        "\n",
        "def evaluate(split, model_path, diagnosis, use_gpu):\n",
        "    train_loader, valid_loader, test_loader = load_data(diagnosis, use_gpu)\n",
        "\n",
        "    model = MRI_alex()\n",
        "    state_dict = torch.load(model_path, map_location=(None if use_gpu else 'cpu'))\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "\n",
        "    if split == 'train':\n",
        "        loader = train_loader\n",
        "    elif split == 'valid':\n",
        "        loader = valid_loader\n",
        "    elif split == 'test':\n",
        "        loader = test_loader\n",
        "    else:\n",
        "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
        "\n",
        "    loss, auc, preds, labels = run_model(model, loader)\n",
        "\n",
        "    print(f'{split} loss: {loss:0.4f}')\n",
        "    print(f'{split} AUC: {auc:0.4f}')\n",
        "\n",
        "    return preds, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdGtOCLJb87S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rundir, task, epochs, learning_rate, use_gpu,model,abnormal_model_path=None):\n",
        "    train_loader, valid_loader = load_data(task, use_gpu)\n",
        "    \n",
        "    model = model\n",
        "    '''for dirpath, dirnames, files in os.walk(rundir):\n",
        "        if not files:\n",
        "            break\n",
        "        max_epoch = 0\n",
        "        model_path = None\n",
        "        for fname in files:\n",
        "            if fname.endswith(\".json\"):\n",
        "                continue\n",
        "            ep = int(fname[27:])\n",
        "            if ep >= max_epoch:\n",
        "                max_epoch = ep\n",
        "                model_path = os.path.join(dirpath, fname)\n",
        "        \n",
        "        if model_path:\n",
        "            state_dict = torch.load(model_path, map_location=(None if use_gpu else 'cpu'))\n",
        "            model.load_state_dict(state_dict)\n",
        "'''\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.3, threshold=1e-4)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        change = datetime.now() - start_time\n",
        "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
        "        \n",
        "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer,abnormal_model_path=abnormal_model_path)\n",
        "\n",
        "        print(f'train loss: {train_loss:0.4f}')\n",
        "        print(f'train AUC: {train_auc:0.4f}')\n",
        "\n",
        "        val_loss, val_auc, _, _ = run_model(model, valid_loader,abnormal_model_path=abnormal_model_path)\n",
        "        \n",
        "        print(f'valid loss: {val_loss:0.4f}')\n",
        "        print(f'valid AUC: {val_auc:0.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-ikFcrcyXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_acl=MRI_alex(training=True)\n",
        "model_men=MRI_alex(training=True)\n",
        "model_ab=MRI_alex(training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hgJjeZhLTd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ce84192-025d-4c1f-c86c-e81e752d31f6"
      },
      "source": [
        "train('/content/drive/My Drive/MRNet-v1.0/Untitled Folder',task='abnormal',epochs=20,learning_rate=1e-5,use_gpu='cuda',model=model_ab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 1. time passed: 0:00:00.000014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [04:31<00:00,  4.16it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.1773\n",
            "train AUC: 0.7710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:32<00:00,  3.66it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1506\n",
            "valid AUC: 0.9453\n",
            "starting epoch 2. time passed: 0:05:04.407229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:38<00:00,  5.18it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.1300\n",
            "train AUC: 0.8922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:29<00:00,  4.08it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1194\n",
            "valid AUC: 0.9499\n",
            "starting epoch 3. time passed: 0:09:11.912287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:38<00:00,  5.16it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.1128\n",
            "train AUC: 0.9202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:35<00:00,  3.36it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1312\n",
            "valid AUC: 0.9486\n",
            "starting epoch 4. time passed: 0:13:26.537929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [04:03<00:00,  4.64it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.1000\n",
            "train AUC: 0.9381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:52<00:00,  2.29it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.2526\n",
            "valid AUC: 0.9499\n",
            "starting epoch 5. time passed: 0:18:22.693117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:41<00:00,  5.09it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0974\n",
            "train AUC: 0.9420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:36<00:00,  3.32it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1174\n",
            "valid AUC: 0.9482\n",
            "starting epoch 6. time passed: 0:22:40.843915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:38<00:00,  5.17it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0896\n",
            "train AUC: 0.9523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:30<00:00,  3.93it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1034\n",
            "valid AUC: 0.9512\n",
            "starting epoch 7. time passed: 0:26:49.891840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:35<00:00,  5.24it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0858\n",
            "train AUC: 0.9566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:27<00:00,  4.30it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1945\n",
            "valid AUC: 0.9503\n",
            "starting epoch 8. time passed: 0:30:53.346566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:36<00:00,  5.21it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0845\n",
            "train AUC: 0.9577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:27<00:00,  4.40it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1541\n",
            "valid AUC: 0.9436\n",
            "starting epoch 9. time passed: 0:34:57.580602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:39<00:00,  5.16it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0871\n",
            "train AUC: 0.9545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:27<00:00,  4.41it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1011\n",
            "valid AUC: 0.9486\n",
            "starting epoch 10. time passed: 0:39:03.944255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:38<00:00,  5.16it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0886\n",
            "train AUC: 0.9529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:29<00:00,  4.08it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1061\n",
            "valid AUC: 0.9423\n",
            "starting epoch 11. time passed: 0:43:12.386576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:34<00:00,  5.26it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0854\n",
            "train AUC: 0.9548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:29<00:00,  4.01it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1973\n",
            "valid AUC: 0.9448\n",
            "starting epoch 12. time passed: 0:47:17.270944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:35<00:00,  5.24it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0877\n",
            "train AUC: 0.9538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:27<00:00,  4.30it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1415\n",
            "valid AUC: 0.9427\n",
            "starting epoch 13. time passed: 0:51:20.883387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:34<00:00,  5.28it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0888\n",
            "train AUC: 0.9525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:28<00:00,  4.17it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1235\n",
            "valid AUC: 0.9461\n",
            "starting epoch 14. time passed: 0:55:23.779508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:33<00:00,  5.29it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0863\n",
            "train AUC: 0.9564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:26<00:00,  4.45it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1920\n",
            "valid AUC: 0.9503\n",
            "starting epoch 15. time passed: 0:59:24.403290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:34<00:00,  5.27it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0886\n",
            "train AUC: 0.9528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:27<00:00,  4.32it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1133\n",
            "valid AUC: 0.9436\n",
            "starting epoch 16. time passed: 1:03:26.746550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:34<00:00,  5.26it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0723\n",
            "train AUC: 0.9734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:28<00:00,  4.27it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1458\n",
            "valid AUC: 0.9432\n",
            "starting epoch 17. time passed: 1:07:29.750647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:40<00:00,  5.13it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0718\n",
            "train AUC: 0.9726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:33<00:00,  3.53it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1022\n",
            "valid AUC: 0.9465\n",
            "starting epoch 18. time passed: 1:11:43.968850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:45<00:00,  5.02it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0718\n",
            "train AUC: 0.9717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:29<00:00,  4.10it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1090\n",
            "valid AUC: 0.9461\n",
            "starting epoch 19. time passed: 1:15:58.433082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:36<00:00,  5.22it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0724\n",
            "train AUC: 0.9718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:28<00:00,  4.22it/s]\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1413\n",
            "valid AUC: 0.9448\n",
            "starting epoch 20. time passed: 1:20:03.547185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1130/1130 [03:40<00:00,  5.11it/s]\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.0714\n",
            "train AUC: 0.9722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [00:28<00:00,  4.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 0.1737\n",
            "valid AUC: 0.9465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIAfJkIRCyKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_ab.state_dict(), '/content/drive/My Drive/MRNet-v1.0/models/abnormal2.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6HZ677rMd9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PazjRUvTS-eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(series):\n",
        "        pad = int((series.shape[2] - INPUT_DIM)/2)\n",
        "        series = series[:,pad:-pad,pad:-pad]\n",
        "        series = (series-np.min(series))/(np.max(series)-np.min(series))*MAX_PIXEL_VAL\n",
        "        series = (series - MEAN) / STDDEV\n",
        "        series = np.stack((series,)*3, axis=1)\n",
        "        series_float = torch.FloatTensor(series)\n",
        "        return series_float\n",
        "\n",
        "def get_study(axial_path, sagit_path, coron_path):\n",
        "    vol_axial = np.load(axial_path)\n",
        "    vol_sagit = np.load(sagit_path)\n",
        "    vol_coron = np.load(coron_path)\n",
        "    # axial\n",
        "    vol_axial_tensor = preprocess(vol_axial)\n",
        "    # sagittal\n",
        "    vol_sagit_tensor =preprocess(vol_sagit)\n",
        "    # coronal\n",
        "    vol_coron_tensor = preprocess(vol_coron)\n",
        "    \n",
        "    return {\"axial\": vol_axial_tensor,\n",
        "            \"sagit\": vol_sagit_tensor,\n",
        "            \"coron\": vol_coron_tensor}\n",
        "\n",
        "\n",
        "def get_prediction(model, tensors, abnormality_prior=None):\n",
        "    vol_axial = tensors[\"axial\"].cuda()\n",
        "    vol_sagit = tensors[\"sagit\"].cuda()\n",
        "    vol_coron = tensors[\"coron\"].cuda()\n",
        "\n",
        "    vol_axial = Variable(vol_axial)\n",
        "    vol_sagit = Variable(vol_sagit)\n",
        "    vol_coron = Variable(vol_coron)\n",
        "\n",
        "    logit = model.forward(vol_axial, vol_sagit, vol_coron)\n",
        "    pred = torch.sigmoid(logit)\n",
        "    pred_npy = pred.data.cpu().numpy()[0][0]\n",
        "    \n",
        "    if abnormality_prior:\n",
        "        pred_npy = pred_npy * abnormality_prior\n",
        "\n",
        "    return pred_npy\n",
        "\n",
        "\n",
        "def lets_predict3()\n",
        "    # Assuming that the input csv has all three views for each ID.\n",
        "    # And that entries are sorted by ID.\n",
        "    views = []\n",
        "    for i, fpath in enumerate(open(input_csv_path).readlines()):\n",
        "        if \"axial\" in fpath:\n",
        "            axial_path = fpath.strip()\n",
        "        elif \"sagittal\" in fpath:\n",
        "            sagit_path = fpath.strip()\n",
        "        elif \"coronal\" in fpath:\n",
        "            coron_path = fpath.strip()\n",
        "        if i%3==2:\n",
        "            views.append(get_study(axial_path, sagit_path, coron_path))\n",
        "\n",
        "\n",
        "    # Loading all models\n",
        "    abnormal_model_path = \"src/abnormal_triple_alex/val0.1071_train0.0868_epoch8\"\n",
        "    acl_model_path = \"src/acl_triple_alex/val0.1310_train0.0504_epoch30\"\n",
        "    meniscal_model_path = \"src/meniscal_triple_alex/val0.2645_train0.1142_epoch22\"\n",
        "\n",
        "\n",
        "    # Getting predictions\n",
        "    abnormality = []\n",
        "    acl_tear = []\n",
        "    meniscal_tear = []\n",
        "    \n",
        "    abnormal_model = MRI_alex( training=False)\n",
        "    state_dict = torch.load(abnormal_model_path)\n",
        "    abnormal_model.load_state_dict(state_dict)\n",
        "    abnormal_model.cuda()\n",
        "    abnormal_model.eval()\n",
        "    for study in views:\n",
        "        abnormality.append(get_prediction(\n",
        "                abnormal_model,\n",
        "                study,\n",
        "                abnormality_prior=None))\n",
        "    del abnormal_model\n",
        "\n",
        "    acl_model = MRI_alex( training=False)\n",
        "    state_dict = torch.load(acl_model_path)\n",
        "    acl_model.load_state_dict(state_dict)\n",
        "    acl_model.cuda()\n",
        "    acl_model.eval()\n",
        "    for idx,study in enumerate(views):\n",
        "        acl_tear.append(get_prediction(\n",
        "                acl_model,\n",
        "                study,\n",
        "                abnormality_prior=abnormality[idx]))\n",
        "    del acl_model\n",
        "\n",
        "    meniscal_model = MRI_alex(training=False)\n",
        "    state_dict = torch.load(meniscal_model_path)\n",
        "    meniscal_model.load_state_dict(state_dict)\n",
        "    meniscal_model.cuda()\n",
        "    meniscal_model.eval()\n",
        "    for idx,study in enumerate(views):\n",
        "        meniscal_tear.append(get_prediction(\n",
        "                meniscal_model,\n",
        "                study,\n",
        "                abnormality_prior=abnormality[idx]))\n",
        "    del meniscal_model\n",
        "    \n",
        "\n",
        "    with open(preds_csv_path, \"w\") as csv_file:        \n",
        "        for i in range(len(abnormality)):\n",
        "            csv_file.write(\",\".join(\n",
        "                [str(abnormality[i]), str(acl_tear[i]), str(meniscal_tear[i])]))\n",
        "            csv_file.write(\"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEfoW144OK8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afb8d196-56a8-4d3c-8583-30749cc5b426"
      },
      "source": [
        "get_prediction(model_ab,get_study(\"/content/drive/My Drive/MRNet-v1.0/valid/axial/1135.npy\",\"/content/drive/My Drive/MRNet-v1.0/valid/sagittal/1135.npy\",\"/content/drive/My Drive/MRNet-v1.0/valid/coronal/1135.npy\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6930317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}