{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of triple MRInet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM12v55IfeY2Ux3Wibf4Fww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadtarek98/Graduation-project/blob/master/notebook%5Ctriple_MRInet_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUCY8w-fksCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "befb6658-feaf-4040-9b6c-acb84592dd93"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaYZrBUSxL8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "3cddf842-b267-491c-d341-9fb2825de19d"
      },
      "source": [
        "!pip install git+https://github.com/ncullen93/torchsample\n",
        "!pip install tensorboardX \n",
        "!pip install --upgrade torch\n",
        "!pip install --upgrade numpy\n",
        "!wget https://gist.githubusercontent.com/archie9211/ae3c8411da88ae8b2a05b0ee1a4fd412/raw/ee1a6e78fc498ad6d4830cd2eb6033839235ea8a/colab-ssh-jupyter.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ncullen93/torchsample\n",
            "  Cloning https://github.com/ncullen93/torchsample to /tmp/pip-req-build-ddb8e75s\n",
            "  Running command git clone -q https://github.com/ncullen93/torchsample /tmp/pip-req-build-ddb8e75s\n",
            "Requirement already satisfied (use --upgrade to upgrade): torchsample==0.1.3 from git+https://github.com/ncullen93/torchsample in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: torchsample\n",
            "  Building wheel for torchsample (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchsample: filename=torchsample-0.1.3-cp36-none-any.whl size=43417 sha256=f053da3618e4ae441a6f354761c737d3c15c1a3bc9ef994e4521b27f7eb47326\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ev_bsv2e/wheels/88/c7/72/14cd9a173eed1e29d0b17d866e7d9ee511d31a834aedd27489\n",
            "Successfully built torchsample\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.1.1)\n",
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "--2020-06-07 19:19:21--  https://gist.githubusercontent.com/archie9211/ae3c8411da88ae8b2a05b0ee1a4fd412/raw/ee1a6e78fc498ad6d4830cd2eb6033839235ea8a/colab-ssh-jupyter.sh\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1125 (1.1K) [text/plain]\n",
            "Saving to: ‘colab-ssh-jupyter.sh.2’\n",
            "\n",
            "colab-ssh-jupyter.s 100%[===================>]   1.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-07 19:19:21 (54.6 MB/s) - ‘colab-ssh-jupyter.sh.2’ saved [1125/1125]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Gn-LUvcVnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import pdb\n",
        "from torch.autograd import Variable\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchsample.transforms import RandomRotate, RandomTranslate, RandomFlip, ToTensor, Compose, RandomAffine\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from sklearn import metrics\n",
        "import pdb\n",
        "from sklearn import metrics\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from sklearn import metrics\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikRxK7f1bm8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = models.alexnet(pretrained=True)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.squeeze(x, dim=0) # only batch size 1 supported\n",
        "        x = self.model.features(x)\n",
        "        x = self.gap(x).view(x.size(0), -1)\n",
        "        x = torch.max(x, 0, keepdim=True)[0]\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class TripleMRNet(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet18\", training=True):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        if self.backbone == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=training)\n",
        "            modules = list(resnet.children())[:-1]\n",
        "            self.axial_net = nn.Sequential(*modules)\n",
        "            for param in self.axial_net.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.axial_net = models.alexnet(pretrained=training)\n",
        "\n",
        "        if self.backbone == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=training)\n",
        "            modules = list(resnet.children())[:-1]\n",
        "            self.sagit_net = nn.Sequential(*modules)\n",
        "            for param in self.sagit_net.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.sagit_net = models.alexnet(pretrained=training)\n",
        "        \n",
        "        if self.backbone == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=training)\n",
        "            modules = list(resnet.children())[:-1]\n",
        "            self.coron_net = nn.Sequential(*modules)\n",
        "            for param in self.coron_net.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.coron_net = models.alexnet(pretrained=training)\n",
        "\n",
        "        self.gap_axial = nn.AdaptiveAvgPool2d(1)\n",
        "        self.gap_sagit = nn.AdaptiveAvgPool2d(1)\n",
        "        self.gap_coron = nn.AdaptiveAvgPool2d(1)\n",
        "       \n",
        "        if self.backbone == \"resnet18\":\n",
        "            self.classifier = nn.Linear(3*512, 1)\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            self.classifier = nn.Linear(3*256, 1)\n",
        "\n",
        "    def forward(self, vol_axial, vol_sagit, vol_coron):\n",
        "        vol_axial = torch.squeeze(vol_axial, dim=0)\n",
        "        vol_sagit = torch.squeeze(vol_sagit, dim=0)\n",
        "        vol_coron = torch.squeeze(vol_coron, dim=0)\n",
        "       \n",
        "        if self.backbone == \"resnet18\":\n",
        "            vol_axial = self.axial_net(vol_axial)\n",
        "            vol_sagit = self.sagit_net(vol_sagit)\n",
        "            vol_coron = self.coron_net(vol_coron)\n",
        "        elif self.backbone == \"alexnet\":\n",
        "            vol_axial = self.axial_net.features(vol_axial)\n",
        "            vol_sagit = self.sagit_net.features(vol_sagit)\n",
        "            vol_coron = self.coron_net.features(vol_coron)\n",
        "\n",
        "        vol_axial = self.gap_axial(vol_axial).view(vol_axial.size(0), -1)\n",
        "        x = torch.max(vol_axial, 0, keepdim=True)[0]\n",
        "        vol_sagit = self.gap_sagit(vol_sagit).view(vol_sagit.size(0), -1)\n",
        "        y = torch.max(vol_sagit, 0, keepdim=True)[0]\n",
        "        vol_coron = self.gap_coron(vol_coron).view(vol_coron.size(0), -1)\n",
        "        z = torch.max(vol_coron, 0, keepdim=True)[0]\n",
        "\n",
        "        w = torch.cat((x, y, z), 1)\n",
        "        out = self.classifier(w)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VI6cA9wDvDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MRI_alex(nn.Module):\n",
        "  def __init__(self, training=True):\n",
        "    super().__init__()\n",
        "    self.axial_net = models.alexnet(pretrained=training)\n",
        "    self.sagit_net = models.alexnet(pretrained=training)\n",
        "    self.coron_net = models.alexnet(pretrained=training)\n",
        "\n",
        "    self.gap_axial = nn.AdaptiveAvgPool2d(1)\n",
        "    self.gap_sagit = nn.AdaptiveAvgPool2d(1)\n",
        "    self.gap_coron = nn.AdaptiveAvgPool2d(1)\n",
        "    self.classifier = nn.Linear(3*256, 1)\n",
        "\n",
        "  def forward(self,vol_axial, vol_sagit, vol_coron):\n",
        "    vol_axial = torch.squeeze(vol_axial, dim=0)\n",
        "    vol_sagit = torch.squeeze(vol_sagit, dim=0)\n",
        "    vol_coron = torch.squeeze(vol_coron, dim=0)\n",
        "\n",
        "    vol_axial = self.axial_net.features(vol_axial)\n",
        "    vol_sagit = self.sagit_net.features(vol_sagit)\n",
        "    vol_coron = self.coron_net.features(vol_coron)\n",
        "\n",
        "    vol_axial = self.gap_axial(vol_axial).view(vol_axial.size(0), -1)\n",
        "    x = torch.max(vol_axial, 0, keepdim=True)[0]\n",
        "\n",
        "    vol_sagit = self.gap_sagit(vol_sagit).view(vol_sagit.size(0), -1)\n",
        "    y = torch.max(vol_sagit, 0, keepdim=True)[0]\n",
        "\n",
        "    vol_coron = self.gap_coron(vol_coron).view(vol_coron.size(0), -1)\n",
        "    z = torch.max(vol_coron, 0, keepdim=True)[0]\n",
        "\n",
        "    w = torch.cat((x, y, z), 1)\n",
        "    out = self.classifier(w)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cVk6Ep0b2nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = 224\n",
        "MAX_PIXEL_VAL = 255\n",
        "MEAN = 58.09\n",
        "STDDEV = 49.73\n",
        "def preprocess(series,transform):\n",
        "   series = torch.tensor(np.stack((series,)*3, axis=1))\n",
        "   if transform is not None:\n",
        "        for i, slice in enumerate(series.split(1)):\n",
        "            series[i] = transform(slice.squeeze())\n",
        "   series = (series - series.min()) / (series.max() - series.min()) * MAX_PIXEL_VAL\n",
        "   series = (series - MEAN) / STDDEV\n",
        "   return series\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, datadir, tear_type, use_gpu,augmentor):\n",
        "        super().__init__()\n",
        "        self.use_gpu = use_gpu\n",
        "        label_dict = {}\n",
        "        self.paths = []\n",
        "        abnormal_label_dict = {}\n",
        "        self.transform=augmentor\n",
        "        if datadir[-1]==\"/\":\n",
        "            datadir = datadir[:-1]\n",
        "        self.datadir = datadir\n",
        "        for i, line in enumerate(open(datadir+'-'+tear_type+'.csv').readlines()):\n",
        "            line = line.strip().split(',')\n",
        "            filename = line[0]\n",
        "            label = line[1]\n",
        "            label_dict[filename] = int(label)\n",
        "        \n",
        "\n",
        "        for i, line in enumerate(open(datadir+'-'+\"abnormal\"+'.csv').readlines()):\n",
        "            line = line.strip().split(',')\n",
        "            filename = line[0]\n",
        "            label = line[1]\n",
        "            abnormal_label_dict[filename] = int(label)\n",
        "\n",
        "        for filename in os.listdir(os.path.join(datadir, \"axial\")):\n",
        "            if filename.endswith(\".npy\"):\n",
        "                self.paths.append(filename)\n",
        "        \n",
        "        self.labels = [label_dict[path.split(\".\")[0]] for path in self.paths]\n",
        "        self.abnormal_labels = [abnormal_label_dict[path.split(\".\")[0]] for path in self.paths]\n",
        "\n",
        "        if tear_type != \"abnormal\":\n",
        "            temp_labels = [self.labels[i] for i in range(len(self.labels)) if self.abnormal_labels[i]==1]\n",
        "            neg_weight = np.mean(temp_labels)\n",
        "        else:\n",
        "            neg_weight = np.mean(self.labels)\n",
        "        \n",
        "        self.weights = [neg_weight, 1 - neg_weight]\n",
        "\n",
        "    def weighted_loss(self, prediction, target):\n",
        "        weights_npy = np.array([self.weights[int(t[0])] for t in target.data])\n",
        "        weights_tensor = torch.FloatTensor(weights_npy)\n",
        "        if self.use_gpu:\n",
        "            weights_tensor = weights_tensor.cuda()\n",
        "        loss = F.binary_cross_entropy_with_logits(prediction, target, weight=Variable(weights_tensor))\n",
        "        return loss\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.paths[index]\n",
        "        vol_axial = np.load(os.path.join(self.datadir, \"axial\", filename))\n",
        "        vol_sagit = np.load(os.path.join(self.datadir, \"sagittal\", filename))\n",
        "        vol_coron = np.load(os.path.join(self.datadir, \"coronal\", filename))\n",
        "\n",
        "        # axial\n",
        "        pad = int((vol_axial.shape[2] - INPUT_DIM)/2)\n",
        "        vol_axial = vol_axial[:,pad:-pad,pad:-pad]\n",
        "        #vol_axial = (vol_axial-np.min(vol_axial))/(np.max(vol_axial)-np.min(vol_axial))*MAX_PIXEL_VAL\n",
        "        vol_axial=preprocess(vol_axial,self.transform)\n",
        "        #vol_axial = (vol_axial - MEAN) / STDDEV\n",
        "        #vol_axial = np.stack((vol_axial,)*3, axis=1)\n",
        "        vol_axial_tensor = torch.FloatTensor(vol_axial)\n",
        "        \n",
        "        \n",
        "        # sagittal\n",
        "        pad = int((vol_sagit.shape[2] - INPUT_DIM)/2)\n",
        "        vol_sagit = vol_sagit[:,pad:-pad,pad:-pad]\n",
        "        #vol_sagit = (vol_sagit-np.min(vol_sagit))/(np.max(vol_sagit)-np.min(vol_sagit))*MAX_PIXEL_VAL\n",
        "        #vol_sagit = (vol_sagit - MEAN) / STDDEV\n",
        "        #vol_sagit = np.stack((vol_sagit,)*3, axis=1)\n",
        "        vol_sagit=preprocess(vol_sagit,self.transform)\n",
        "        vol_sagit_tensor = torch.FloatTensor(vol_sagit)\n",
        "\n",
        "        # coronal\n",
        "        pad = int((vol_coron.shape[2] - INPUT_DIM)/2)\n",
        "        vol_coron = vol_coron[:,pad:-pad,pad:-pad]\n",
        "        vol_coron=preprocess(vol_coron,self.transform)\n",
        "        #vol_coron = (vol_coron - MEAN) / STDDEV\n",
        "        #vol_coron = np.stack((vol_coron,)*3, axis=1)\n",
        "        vol_coron_tensor = torch.FloatTensor(vol_coron)\n",
        "\n",
        "        label_tensor = torch.FloatTensor([self.labels[index]])\n",
        "\n",
        "\n",
        "\n",
        "        return vol_axial_tensor, vol_sagit_tensor, vol_coron_tensor, label_tensor, self.abnormal_labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "def load_data(task, use_gpu):\n",
        "    train_dir = \"/content/drive/My Drive/MRNet-v1.0/train\"\n",
        "    valid_dir = \"/content/drive/My Drive/MRNet-v1.0/valid\"\n",
        "    train_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomAffine(25, translate=(0.1, 0.1)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    valid_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    train_dataset = Dataset(train_dir, task, use_gpu,train_transform)\n",
        "    valid_dataset = Dataset(valid_dir, task, use_gpu,valid_transform)\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=1, num_workers=0, shuffle=True)\n",
        "    valid_loader = data.DataLoader(valid_dataset, batch_size=1, num_workers=0, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2F9XbQrcQYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, loader, train=False, optimizer=None,abnormal_model_path=None):\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        if abnormal_model_path:\n",
        "            abnormal_model = TripleMRNet(backbone=model.backbone)\n",
        "            state_dict = torch.load(abnormal_model_path)\n",
        "            abnormal_model.load_state_dict(state_dict)\n",
        "            abnormal_model.cuda()\n",
        "            abnormal_model.eval()\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in tqdm(loader):\n",
        "        vol_axial, vol_sagit, vol_coron, label, abnormal = batch\n",
        "        \n",
        "        if train:\n",
        "            if abnormal_model_path and not abnormal:\n",
        "                continue\n",
        "            optimizer.zero_grad()\n",
        "        if loader.dataset.use_gpu:\n",
        "            vol_axial, vol_sagit, vol_coron = vol_axial.cuda(), vol_sagit.cuda(), vol_coron.cuda()\n",
        "            label = label.cuda()\n",
        "        vol_axial, vol_sagit, vol_coron = Variable(vol_axial), Variable(vol_sagit), Variable(vol_coron)\n",
        "        label = Variable(label)\n",
        "\n",
        "        logit = model.forward(vol_axial, vol_sagit, vol_coron)\n",
        "\n",
        "        loss = loader.dataset.weighted_loss(logit, label)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pred = torch.sigmoid(logit)\n",
        "\n",
        "        pred_npy = pred.data.cpu().numpy()[0][0]\n",
        "\n",
        "        if abnormal_model_path and not train:\n",
        "            abnormal_logit = abnormal_model.forward(vol_axial,vol_sagit,vol_coron)\n",
        "            abnormal_pred = torch.sigmoid(abnormal_logit)\n",
        "            abnormal_pred_npy = abnormal_pred.data.cpu().numpy()[0][0]\n",
        "            pred_npy = pred_npy * abnormal_pred_npy\n",
        "\n",
        "        label_npy = label.data.cpu().numpy()[0][0]\n",
        "\n",
        "        preds.append(pred_npy)\n",
        "        labels.append(label_npy)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    \n",
        "    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    if abnormal_model_path and not train:\n",
        "        del abnormal_model\n",
        "\n",
        "    return avg_loss, auc, preds, labels\n",
        "\n",
        "def evaluate(split, model_path, diagnosis, use_gpu):\n",
        "    train_loader, valid_loader, test_loader = load_data(diagnosis, use_gpu)\n",
        "\n",
        "    model = MRNet()\n",
        "    state_dict = torch.load(model_path, map_location=(None if use_gpu else 'cpu'))\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "\n",
        "    if split == 'train':\n",
        "        loader = train_loader\n",
        "    elif split == 'valid':\n",
        "        loader = valid_loader\n",
        "    elif split == 'test':\n",
        "        loader = test_loader\n",
        "    else:\n",
        "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
        "\n",
        "    loss, auc, preds, labels = run_model(model, loader)\n",
        "\n",
        "    print(f'{split} loss: {loss:0.4f}')\n",
        "    print(f'{split} AUC: {auc:0.4f}')\n",
        "\n",
        "    return preds, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdGtOCLJb87S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rundir, task, epochs, learning_rate, use_gpu,model,abnormal_model_path=None):\n",
        "    train_loader, valid_loader = load_data(task, use_gpu)\n",
        "    \n",
        "    model = model\n",
        "    '''for dirpath, dirnames, files in os.walk(rundir):\n",
        "        if not files:\n",
        "            break\n",
        "        max_epoch = 0\n",
        "        model_path = None\n",
        "        for fname in files:\n",
        "            if fname.endswith(\".json\"):\n",
        "                continue\n",
        "            ep = int(fname[27:])\n",
        "            if ep >= max_epoch:\n",
        "                max_epoch = ep\n",
        "                model_path = os.path.join(dirpath, fname)\n",
        "        \n",
        "        if model_path:\n",
        "            state_dict = torch.load(model_path, map_location=(None if use_gpu else 'cpu'))\n",
        "            model.load_state_dict(state_dict)\n",
        "'''\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.3, threshold=1e-4)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        change = datetime.now() - start_time\n",
        "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
        "        \n",
        "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer,abnormal_model_path=abnormal_model_path)\n",
        "\n",
        "        print(f'train loss: {train_loss:0.4f}')\n",
        "        print(f'train AUC: {train_auc:0.4f}')\n",
        "\n",
        "        val_loss, val_auc, _, _ = run_model(model, valid_loader,abnormal_model_path=abnormal_model_path)\n",
        "        \n",
        "        print(f'valid loss: {val_loss:0.4f}')\n",
        "        print(f'valid AUC: {val_auc:0.4f}')\n",
        "\n",
        "        scheduler.step(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-ikFcrcyXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_acl=MRI_alex(training=True)\n",
        "model_men=MRI_alex(training=True)\n",
        "model_ab=MRI_alex(training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hgJjeZhLTd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "c9ee1616-7eac-4113-c82d-4c0c37bbcec0"
      },
      "source": [
        "train('/content/drive/My Drive/MRNet-v1.0/Untitled Folder',task='acl',epochs=20,learning_rate=1e-5,use_gpu='cuda',model=model_acl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/1130 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 1. time passed: 0:00:00.000007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 1/1130 [00:02<46:53,  2.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 2/1130 [00:04<44:35,  2.37s/it]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 3/1130 [00:06<43:53,  2.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 4/1130 [00:09<46:12,  2.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 5/1130 [00:11<44:46,  2.39s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 6/1130 [00:14<47:24,  2.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 7/1130 [00:16<46:11,  2.47s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 8/1130 [00:18<42:50,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 9/1130 [00:21<43:38,  2.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 10/1130 [00:23<41:14,  2.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 11/1130 [00:25<44:17,  2.38s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 12/1130 [00:28<47:15,  2.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 13/1130 [00:30<43:35,  2.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 14/1130 [00:32<41:54,  2.25s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|▏         | 15/1130 [00:34<40:28,  2.18s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|▏         | 16/1130 [00:37<40:51,  2.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 17/1130 [00:39<40:13,  2.17s/it]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 18/1130 [00:41<40:00,  2.16s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6a3367c1ed7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/MRNet-v1.0/Untitled Folder'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_acl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-95aec5d966d9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rundir, task, epochs, learning_rate, use_gpu, model, abnormal_model_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting epoch {}. time passed: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mabnormal_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabnormal_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train loss: {train_loss:0.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-36ce74110638>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, loader, train, optimizer, abnormal_model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mvol_axial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol_sagit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol_coron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIAfJkIRCyKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_acl.state_dict(), '/content/drive/My Drive/MRNet-v1.0/Untitled Folder/acl.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6HZ677rMd9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf62d25f-b6c0-44b6-e81d-7424e1bf6d70"
      },
      "source": [
        "model_acl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MRI_alex(\n",
              "  (axial_net): AlexNet(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (9): ReLU(inplace=True)\n",
              "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "    (classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (sagit_net): AlexNet(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (9): ReLU(inplace=True)\n",
              "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "    (classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (coron_net): AlexNet(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (7): ReLU(inplace=True)\n",
              "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (9): ReLU(inplace=True)\n",
              "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "    (classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (gap_axial): AdaptiveAvgPool2d(output_size=1)\n",
              "  (gap_sagit): AdaptiveAvgPool2d(output_size=1)\n",
              "  (gap_coron): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ofw1EoMmYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}